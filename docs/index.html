<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>
  body {
    background-color: white;
    padding: 100px;
    width: 1000px;
    margin: auto;
    text-align: left;
    font-weight: 300;
    font-family: 'Open Sans', sans-serif;
    color: #121212;
  }
  h1, h2, h3, h4 {
    font-family: 'Source Sans Pro', sans-serif;
  }
  kbd {
    color: #121212;
  }
</style>
<title>CS 184 Path Tracer</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']]
    }
  };
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>

</head>


<body>

<h1 align="middle">CS 184: Computer Graphics and Imaging, Spring 2023</h1>
<h1 align="middle">Project 3-1: Path Tracer</h1>
<h2 align="middle">Puyuan Yi, Haoda Li</h2>

<!-- Add Website URL -->
<h2 align="middle">Website URL: <a href="TODO">https://cal-cs184-student.github.io/p3-1-pathtracer-sp23-idm_p3/</a></h2>

<br><br>

<h2 align="middle" >Overview</h2>
<p style="line-height:200%">
    In this project, we gained a deep understanding of the whole pipeline of path tracing. To be more specific, we first learned how to write
    the basic intersection of different primitives (triangle and sphere), adjust our camera and generate rays. Later, in order to 
    make our intersection judgement faster, we constructed BVH tree and accelerated our intersection judgement. After finishing all the 
    intersection calculation part, we implemented direct illumination and global illumination based on Monte Carlo integration. Finally, in order to
    improve our rendering quality, we utilized adaptive sampling to reduce the noise in our rendered picture.
</p>
<br>

<h2 align="middle">Part 1: Ray Generation and Scene Intersection (20 Points)</h2>
<!-- Walk through the ray generation and primitive intersection parts of the rendering pipeline.
Explain the triangle intersection algorithm you implemented in your own words.
Show images with normal shading for a few small .dae files. -->

<h3>
  Walk through the ray generation and primitive intersection parts of the rendering pipeline.
</h3>
<ul>
    <li style="line-height:200%">
        To generate a ray, we first find the location (pixel) that we want in our pictures (pixel coordinate). Then we project this location to the camera coordinate by using
        the field of view on two directions. The z value will
        be set to -1 in default and our origin is (0, 0, 0). Now we have our origin and normalized vector in camera coordinate and finally we project the ray to the world coordinate.
        Multiple sampling for a single pixel can be performed by perturbing the ray direction (We can use <b>get_sample()</b> to help use do this). This is
        the <b>Monte Carlo estimation</b>.
    </li>
    <li style="line-height:200%">
        For the primitive intersection, our work is to find the intersection of a given ray and a given object.
        We want to know the intersection point(know the vector t). For objects in different shapes, we have different methods.
        For example, we will use <b>Moller-Trumbore Algorithm</b>to calculate the result.
    </li>
</ul>
<br>

<h3 style="line-height:200%">
  Explain the triangle intersection algorithm you implemented in your own words.
</h3>
<p style="line-height:200%">
    The slides provide us with the solution: We can first do ray intersection with a plane and then judge
    whether our intersection point is inside our triangle. We use <b>Moller-Trumbore Algorithm</b> in this part. This algorithm's basic logic is:
    <ul>
        <li style="line-height:200%">
            Check if the ray is parallel to the triangle. When parallel happens, the dot product between the ray's direction vector 
            and the plane's normal vector will be zero. 
        </li>
        <li style="line-height:200%">
            Check if the ray-plane intersection lies outside the triangle by using barycentric coordinates.
        </li>
    </ul>
    After running algorithm, we will get our result. We will get a t value if our ray is not parallel to the triangle. And the rest b1 and b2
    are barycentric coordinates. If the intersection point is inside the triangle, it should fulfill the requirement:
    <p align="middle"><pre align="middle">b0, b1, b2 in [0, 1], b0 + b1 + b2 = 1</pre></p>
    We can use this to judge whether the ray intersects with the triangle and also calculate the surface normal easily.
</p>
<br>

<h3>
  Show images with normal shading for a few small .dae files.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="part1/1.png" align="middle" width="400px"/>
        <figcaption>sky/CBspheres_lambertian.dae</figcaption>
      </td>
      <td>
        <img src="part1/2.png" align="middle" width="400px"/>
        <figcaption>sky/bunny.dae</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="part1/3.png" align="middle" width="400px"/>
        <figcaption>meshedit/teapot.dae</figcaption>
      </td>
      <td>
        <img src="part1/4.png" align="middle" width="400px"/>
        <figcaption>simple/plane.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>


<h2 align="middle">Part 2: Bounding Volume Hierarchy (20 Points)</h2>
<!-- Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis. -->

<h3>
  Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
</h3>
<p style="line-height:200%">
   The first part and most important part for the BVH algorithm is to construct BVH tree. For leaf nodes in this tree, they
    will save the pointer to the corresponding primitives. For internal nodes, they will save the pointer to their left and right trees. All nodes
    save their bounding box in order to do intersection test.While building the tree, we need to use a heuristic to find the optimal axis to seperate our
    nodes. Our used heuristic is <b>surface area heuristic</b>. During this heuristic, our cost is calculated by the following equation:
    <p align="middle"><pre align="middle">Cost(node) = Cost_trav + SA(L) * Count(L) + SA(R) * Count(R)</pre></p>
    where the SA is the surface area of the bbox, it can be easily calculated by the bbox <b>extent</b> member. After finding the best axis, we utilize
    this axis to divide our node and build our BVH tree. 
</p>

<h3>
  Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="part2/1.png" align="middle" width="400px"/>
        <figcaption>meshedit/maxplanck.dae</figcaption>
      </td>
      <td>
        <img src="part2/2.png" align="middle" width="400px"/>
        <figcaption>sky/CBlucy.dae </figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="part2/3.png" align="middle" width="400px"/>
        <figcaption>sky/dragon.dae</figcaption>
      </td>
      <td>
        <img src="part2/4.png" align="middle" width="400px"/>
        <figcaption>sky/blob.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<h3>
  Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis.
</h3>
<p style="line-height:200%">
    Here is the result table that comparing rendering times on a few scenes with and without BVH acceleration. The first line is the selected scenes, 
    the second line is the rending time use BVH construction and the third line without. We can notice that for the simple scenes, such as cube.dae, the time
    difference is small and even without BVH is faster than BVH (because we do not need to build trees.) But for the larger scens, the rendering with BVH
    acceleration is much faster!
</p>
<div style="text-align: center;">
    <table style="width: 100%;">
        <thead>
            <tr>
                <th>sky/bunny.dae</th>
                <th>meshedit/cow.dae</th>
                <th>simple/cube.dae</th>
                <th>sky/CBlucy.dae</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>0.3631s</td>
                <td>0.3163s</td>
                <td>0.1338s</td>
                <td>0.3705s</td>
            </tr>
            <tr>
                <td>204.36s</td>
                <td>31.45s</td>
                <td>0.1203s</td>
                <td>878.64s</td>
            </tr>
        </tbody>
    </table>
</div>
<br>

<h2 align="middle">Part 3: Direct Illumination (20 Points)</h2>
<!-- Walk through both implementations of the direct lighting function.
Show some images rendered with both implementations of the direct lighting function.
Focus on one particular scene with at least one area light and compare the noise levels in soft shadows when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, not uniform hemisphere sampling.
Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis. -->

<h3>
  Walk through both implementations of the direct lighting function.
</h3>
<p style="line-height:200%">
    In this part, we implemented uniform hemisphere sampling and lighting sampling, here is how we implemented them:
    ·
</p>

<h3>
  Show some images rendered with both implementations of the direct lighting function.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <!-- Header -->
    <tr align="center">
      <th>
        <b>Uniform Hemisphere Sampling</b>
      </th>
      <th>
        <b>Light Sampling</b>
      </th>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="part3/1_1.png" align="middle" width="400px"/>
        <figcaption>sky/CBbunny.dae</figcaption>
      </td>
      <td>
        <img src="part3/1_2.png" align="middle" width="400px"/>
        <figcaption>sky/CBbunny.dae</figcaption>
      </td>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="part3/1_3.png" align="middle" width="400px"/>
        <figcaption>sky/CBspheres_lambertian.dae</figcaption>
      </td>
      <td>
        <img src="part3/1_4.png" align="middle" width="400px"/>
        <figcaption>sky/CBspheres_lambertian.dae</figcaption>
      </td>
    </tr>
    <br>
  </table>
</div>
<br>

<h3>
  Focus on one particular scene with at least one area light and compare the noise levels in <b>soft shadows</b> when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, <b>not</b> uniform hemisphere sampling.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="part3/2_1.png" align="middle" width="200px"/>
        <figcaption>1 Light Ray (sky/dragon.dae)</figcaption>
      </td>
      <td>
        <img src="part3/2_2.png" align="middle" width="200px"/>
        <figcaption>4 Light Rays (sky/dragon.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="part3/2_3.png" align="middle" width="200px"/>
        <figcaption>16 Light Rays (sky/dragon.dae)</figcaption>
      </td>
      <td>
        <img src="part3/2_4.png" align="middle" width="200px"/>
        <figcaption>64 Light Rays (sky/dragon.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<p style="line-height:200%">
    As we can see, the shadows are quite noisy with only 1 light ray and 1 sample per pixel. It looks like the <b>salt-and-pepper</b>
    noise which contains a lot of black and white points on a smooth surface. It is very easy to understand why it happens. When the number of
    sampling rays increase, the variance becomes small and the Monte Carlo becomes more stable. For example, it is hard to find the obvious noise on the
    64-ray sampling result. However, it's worth noting that increasing the number of light rays and samples per pixel comes at the cost of longer render times. So it's often a tradeoff between noise and rendering time.
</p>
<br>

<h3 style="line-height:200%">
  Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis.

</h3>
<p style="line-height:200%">
    In general, lighting sampling can produce smoother and more accurate results than uniform hemisphere sampling (refer to the pictures above),
    .This is because lighting sampling takes into account the actual shape and size of light sources,
    which can greatly affect the distribution of light on surfaces.On the other hand, uniform hemisphere sampling can be faster and easier to implement than lighting sampling,
    and may be sufficient for simple scenes.Overall, the choice between uniform hemisphere sampling and lighting sampling depends on the requirements of the final rendering accuracy, 
    as well as factors such as rendering time and computational resources.
</p>
<br>


<h2 align="middle">Part 4: Global Illumination (20 Points)</h2>
<!-- Walk through your implementation of the indirect lighting function.
Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)
For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.
Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
You will probably want to use the instructional machines for the above renders in order to not burn up your own computer for hours. -->

<h3>
  Walk through your implementation of the indirect lighting function.
</h3>
<p style="line-height:200%">
    Here's a high-level overview of how we implemented the indirect lighting function using path tracing:
    <ul>
        <li style="line-height:200%">
            Trace a primary ray from the camera into the scene.
        </li>
        <li style="line-height:200%">
            At the first surface hit by the primary ray,
            generate a random direction for a secondary ray  with its pdf to be bounced off of the surface.
            We can do this by utilizing <b>sample_f</b> function.
        </li>
        <li style="line-height:200%">
            If the secondary ray hits another surface, repeat these steps
            until the ray eventually get killed by <b>Russian Roulette</b> or <b>reaches a maximum depth</b>.
        </li>
        <li style="line-height:200%">
            Calculating the contribution of indirect lighting using unbiased Monte Claro estimation (divided by p, p is the probability
            of Russian Roulette.)
        </li>
    </ul>
    We run this function recursively to get the indirect lighting!
</p>
<br>

<h3>
  Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="part4/1_1.png" align="middle" width="400px"/>
        <figcaption>sky/CBspheres_lambertian.dae</figcaption>
      </td>
      <td>
        <img src="part4/1_2.png" align="middle" width="400px"/>
        <figcaption>sky/bunny.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<h3>
  Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="part4/2_1.png" align="middle" width="400px"/>
        <figcaption>Only direct illumination (sky/CBspheres_lambertian.dae)</figcaption>
      </td>
      <td>
        <img src="part4/2_2.png" align="middle" width="400px"/>
        <figcaption>Only indirect illumination (sky/CBspheres_lambertian.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p style="line-height:200%">
    It is very easy to understand that the direct illumination result is the same as the result of part3. And all the white pixels in the 
    images are the pixels that can directly get the light from the light source. But for the indirect illumination, the pixels get the light energy from the light
    source after at least one bounce from other objects. For example, for the ceiling area, the direct illumination is completely dark (because we do not add zero bounce illumination in this experiment.)
    But for the indirect illumination, the ceiling will get its indirect light reflected from other objects.
</p>
<br>

<h3>
  For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="part4/3_1.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 0 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="part4/3_2.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 1 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="part4/3_3.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 2 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="part4/3_4.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 3 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="part4/3_5.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 100 (CBbunny.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p style="line-height:200%">
    We have alreadly seen the pictures (m = 0 and m = 1)before, because m = 0 equals to <b>zero_bounce_illumination</b> and m = 1
    equals to <b>one_bound_illumination</b>.And all the rest pictures are <b>global_illumination</b>.It has to be mentioned that the difference
    between global illumination is small, even we increase the max_ray_length from 3 to 100. The only intuitive feeling is that the higher the max_
    ray_length, the brighter the picture. That is because there are so little light information after many rounds of bounces. In summary,
    the choice of our max_ray_length should depend on the requirements of the final rendering accuracy,
    as well as factors such as rendering time and computational resources.
</p>
<br>

<h3>
  Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="part4/4_1.png" align="middle" width="400px"/>
        <figcaption>1 sample per pixel (sky/dragon.dae)</figcaption>
      </td>
      <td>
        <img src="part4/4_2.png" align="middle" width="400px"/>
        <figcaption>2 samples per pixel (sky/dragon.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="part4/4_3.png" align="middle" width="400px"/>
        <figcaption>4 samples per pixel (sky/dragon.dae)</figcaption>
      </td>
      <td>
        <img src="part4/4_4.png" align="middle" width="400px"/>
        <figcaption>8 samples per pixel (sky/dragon.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="part4/4_5.png" align="middle" width="400px"/>
        <figcaption>16 samples per pixel (sky/dragon.dae)</figcaption>
      </td>
      <td>
        <img src="part4/4_6.png" align="middle" width="400px"/>
        <figcaption>64 samples per pixel (sky/dragon.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="part4/4_7.png" align="middle" width="400px"/>
        <figcaption>1024 samples per pixel (sky/dragon.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p style="line-height:200%">
    Here is several main differences with different sample-per-pixel rates:
    <ul>
        <li style="line-height:200%">
            <b>Noise</b>: When rendering with a low sample-per-pixel rate, the image can be noisy. This is 
            because there are not enough samples per pixel to accurately estimate the lighting in the scene, resulting in a less accurate image.
            (Higher the sample rate, lower the variance of Monte Carlo integration)
        </li>
        <li style="line-height:200%">
            <b>Blurring</b>: When rendering with a high sample-per-pixel rate, the image can be overly smooth or blurry. T
            his is because the Monte Carlo integration is averaging over a large number of samples, 
            which can smooth out details and result in a less sharp image.
        </li>
        <li style="line-height:200%">
            <b>Render time</b>: The higher the sample-per-pixel rate, the longer it takes to render the image. 
            This is because the algorithm needs to perform more calculations for each pixel, 
            and thus requires more processing time.For example, it only takes me nearly 1 min to render in 64 sample-per-pixel rate, but nearly 20 mins
            in 1024 sample-per-pixel rate.
        </li>
    </ul>
</p>
<br>


<h2 align="middle">Part 5: Adaptive Sampling (20 Points)</h2>
<!-- Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
Pick one scene and render it with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth. -->

<h3>
  Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
</h3>
<p style="line-height:200%">
    The basic idea of adaptive sampling is to concentrate sampling in areas of the scene that are more important or difficult to render,
    while reducing the number of samples in areas where the image is already well-sampled.
    Our algorithm works by estimating the variance of the illumination at each pixel, and using this information to determine where to stop sampling.
    Areas with high variance will sample many times (up to 2048 in our experiments), while areas with low variance are given fewer samples.
    To be more specific, here is our implementation:
    <ul>
        <li style="line-height:200%">
            Compute the variance and mean of the illumination at each pixel. However, we do not compute for each new sample, we
            do this computation per <b>samplesPerBatch</b>
        </li>
        <li style="line-height:200%">
            If the pixel is already converged, which means this pixel fulfills this equation:
            <p align="middle"><pre align="middle">1.96 * variance / sqrt(sample times) <= maxTolerance * mean</pre></p>
            (In our experiment, maxTolerance is set to 0.5). We stop the sampling for loop immediately.
            
        </li>
    </ul>
</p>
<br>

<h3>
  Pick two scenes and render them with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="part5/1_1.png" align="middle" width="400px"/>
        <figcaption>Rendered image (sky/CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="part5/1_2.png" align="middle" width="400px"/>
        <figcaption>Sample rate image (sky/CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="part5/1_3.png" align="middle" width="400px"/>
        <figcaption>Rendered image (sky/dragon.dae)</figcaption>
      </td>
      <td>
        <img src="part5/1_4.png" align="middle" width="400px"/>
        <figcaption>Sample rate image (sky/dragon.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>


</body>
</html>
